import os
import re
import pandas as pd
from pathlib import Path
import chromadb
from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import torch
import time
from getpass import getpass
from PIL import Image
import google.generativeai as genai
from datetime import datetime
import json
import base64
import io
from dotenv import load_dotenv

# =============================================================================
# C·∫§U H√åNH H·ªÜ TH·ªêNG
# =============================================================================
# S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n ƒë·ªông ƒë·ªÉ tr√°nh l·ªói hardcode C:\FPT...
CURRENT_DIR = Path(__file__).parent.resolve()
CHUNKS_FILE = CURRENT_DIR / "data" / "product_chunks.txt"
PERSIST_DIRECTORY = CURRENT_DIR / "db_chroma_v2"
CHAT_HISTORY_DIR = CURRENT_DIR / "chat_history"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# TEST MODE
TEST_MODE = True  # ƒê·ªïi th√†nh True ƒë·ªÉ test nhanh
MAX_TEST_CHUNKS = 200  # S·ªë chunks d√πng khi TEST_MODE = True

# Global cache cho embeddings
_CACHED_EMBEDDINGS = None

# T·ª∑ gi√° USD ‚Üí VND (c·ªë ƒë·ªãnh)
USD_TO_VND = 26349

# =============================================================================
# DATA MAPPING (M·ªöI)
# =============================================================================
SKIN_CONDITION_TO_SKIN_TYPE = {
    # ‚ö†Ô∏è ∆ØU TI√äN: T·ª´ kh√≥a D√ÄI/C·ª§ TH·ªÇ tr∆∞·ªõc, NG·∫ÆN/CHUNG sau ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    
    # M·ª•n c√≥c (warts) - ∆ØU TI√äN TR∆Ø·ªöC "m·ª•n"
    "warts": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "m·ª•n c√≥c": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "c√≥c": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    # M·ª•n tr·ª©ng c√° - SAU "m·ª•n c√≥c", TR∆Ø·ªöC "m·ª•n"
    "m·ª•n tr·ª©ng c√°": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    # M·ª•n (acne) - CU·ªêI C√ôNG
    "acne": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "m·ª•n": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    # C√°c b·ªánh kh√°c
    "actinic keratosis": ["Kh√¥", "Th∆∞·ªùng"],  # D√†y s·ª´ng
    "da d√†y s·ª´ng": ["Kh√¥", "Th∆∞·ªùng"],
    "d√†y s·ª´ng": ["Kh√¥", "Th∆∞·ªùng"],
    
    "drug eruption": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],  # Ph√°t ban thu·ªëc
    "ph√°t ban do thu·ªëc": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "ph√°t ban thu·ªëc": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "eczema": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],  # Ch√†m
    "ch√†m": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "vi√™m da": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "psoriasis": ["Kh√¥"],  # V·∫£y n·∫øn
    "v·∫£y n·∫øn": ["Kh√¥"],
    
    "rosacea": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],  # Tr·ª©ng c√° ƒë·ªè
    "tr·ª©ng c√° ƒë·ªè": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "da ƒë·ªè": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "seborrheic keratoses": ["Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],  # Vi√™m da ti·∫øt b√£
    "vi√™m da ti·∫øt b√£": ["Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "sun damage": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "Nh·∫°y c·∫£m"],  # T·ªïn th∆∞∆°ng n·∫Øng
    "h∆∞ t·ªïn do n·∫Øng": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "Nh·∫°y c·∫£m"],
    "t·ªïn th∆∞∆°ng n·∫Øng": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "Nh·∫°y c·∫£m"],
    
    "tinea": ["H·ªón h·ª£p", "D·∫ßu"],  # N·∫•m da
    "n·∫•m da": ["H·ªón h·ª£p", "D·∫ßu"],
    "n·∫•m": ["H·ªón h·ª£p", "D·∫ßu"],
}

# Danh s√°ch b·ªánh da ƒë∆∞·ª£c h·ªó tr·ª£ t∆∞ v·∫•n (Gi·ªØ l·∫°i t·ª´ file c≈© ƒë·ªÉ d√πng cho h√†m check c≈©)
SUPPORTED_SKIN_CONDITIONS = [
    "m·ª•n", "acne", "m·ª•n tr·ª©ng c√°",
    "ch√†m", "eczema", "vi√™m da",
    "v·∫£y n·∫øn", "psoriasis",
    "tr·ª©ng c√° ƒë·ªè", "rosacea", "da ƒë·ªè",
    "d√†y s·ª´ng", "actinic keratosis", "da d√†y s·ª´ng",
    "n·∫•m da", "tinea", "n·∫•m",
    "vi√™m da ti·∫øt b√£", "seborrheic keratoses",
    "t·ªïn th∆∞∆°ng n·∫Øng", "sun damage", "h∆∞ t·ªïn do n·∫Øng",
    "m·ª•n c√≥c", "warts", "c√≥c",
    "ph√°t ban thu·ªëc", "drug eruption", "ph√°t ban do thu·ªëc"
]

# =============================================================================
# CORE FUNCTIONS (T·ª™ FILE M·ªöI)
# =============================================================================

def detect_skin_condition_and_types(query):
    """
    Ph√°t hi·ªán b·ªánh da trong c√¢u h·ªèi v√† tr·∫£ v·ªÅ lo·∫°i da ph√π h·ª£p
    Returns: (detected_condition, skin_types_list) ho·∫∑c (None, None)
    
    ‚ö†Ô∏è ∆ØU TI√äN: Ki·ªÉm tra t·ª´ kh√≥a D√ÄI tr∆∞·ªõc (m·ª•n c√≥c) r·ªìi m·ªõi ƒë·∫øn NG·∫ÆN (m·ª•n)
    ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n khi "m·ª•n c√≥c" ch·ª©a t·ª´ "m·ª•n"
    """
    query_lower = query.lower()
    
    # S·∫Øp x·∫øp theo ƒë·ªô d√†i t·ª´ kh√≥a (d√†i -> ng·∫Øn) ƒë·ªÉ ∆∞u ti√™n match c·ª• th·ªÉ tr∆∞·ªõc
    sorted_conditions = sorted(
        SKIN_CONDITION_TO_SKIN_TYPE.items(),
        key=lambda x: len(x[0]),  # S·∫Øp x·∫øp theo ƒë·ªô d√†i t·ª´ kh√≥a
        reverse=True  # T·ª´ d√†i ƒë·∫øn ng·∫Øn
    )
    
    for condition, skin_types in sorted_conditions:
        if condition in query_lower:
            return condition, skin_types
    
    return None, None

def extract_product_name(chunk_text):
    """Tr√≠ch xu·∫•t t√™n s·∫£n ph·∫©m t·ª´ chunk text"""
    # T√¨m "Product Name: ..."
    match = re.search(r'Product Name:\s*(.+?)(?:\n|$)', chunk_text, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # T√¨m "T√™n s·∫£n ph·∫©m: ..."
    match = re.search(r'T√™n s·∫£n ph·∫©m:\s*(.+?)(?:\n|$)', chunk_text, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # Fallback: l·∫•y d√≤ng ƒë·∫ßu ti√™n
    lines = chunk_text.split('\n')
    for line in lines:
        if ':' in line:
            # L·∫•y ph·∫ßn sau d·∫•u : ƒë·∫ßu ti√™n
            potential_name = line.split(':', 1)[1].strip()
            if len(potential_name) > 5:  # T√™n s·∫£n ph·∫©m th∆∞·ªùng d√†i h∆°n 5 k√Ω t·ª±
                return potential_name
    
    return "Unknown Product"

def extract_field_from_chunk(chunk_text, field_name):
    """Tr√≠ch xu·∫•t gi√° tr·ªã c·ªßa field t·ª´ chunk text"""
    # T√¨m "Field_name: value"
    pattern = rf'{field_name}:\s*(.+?)(?:\n|$)'
    match = re.search(pattern, chunk_text, re.IGNORECASE)
    
    if match:
        value = match.group(1).strip()
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng mong mu·ªën
        value = value.replace('---', '').strip()
        if value and value != 'N/A':
            return value
    
    return None

def convert_price_in_text(text):
    """T√¨m v√† chuy·ªÉn ƒë·ªïi gi√° USD sang VND trong text"""
    # T√¨m pattern: $XX ho·∫∑c $XX.XX
    def replace_price(match):
        price_str = match.group(1)
        try:
            price_usd = float(price_str)
            price_vnd = int(price_usd * USD_TO_VND)
            # Format: $XX (‚âà XXX.XXX VND)
            return f"${price_usd:.0f} (‚âà {price_vnd:,} VND)".replace(',', '.')
        except:
            return match.group(0)
    
    # Thay th·∫ø t·∫•t c·∫£ $XX ho·∫∑c $XX.XX
    result = re.sub(r'\$([0-9]+(?:\.[0-9]+)?)', replace_price, text)
    return result

def setup_api_key():
    """Thi·∫øt l·∫≠p Google API Key"""
    if "GOOGLE_API_KEY" not in os.environ:
        print("\nüîë C·∫ßn Google API Key ƒë·ªÉ s·ª≠ d·ª•ng Gemini")
        print("üí° L·∫•y key mi·ªÖn ph√≠ t·∫°i: https://makersuite.google.com/app/apikey\n")
        load_dotenv()
        print("‚úÖ ƒê√£ thi·∫øt l·∫≠p API Key!\n")
    else:
        print("‚úÖ API Key ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh s·∫µn!\n")
    
    # Configure genai for vision
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

def load_or_create_vectorstore():
    """Load vector store c√≥ s·∫µn ho·∫∑c t·∫°o m·ªõi n·∫øu ch∆∞a c√≥, v·ªõi error handling."""
    global _CACHED_EMBEDDINGS
    
    print("=" * 80)
    print("üìö KH·ªûI T·∫†O VECTOR STORE")
    print("=" * 80)
    
    db = None
    embeddings = None
    
    try: # <<< Try ch√≠nh bao quanh to√†n b·ªô h√†m >>>
        
        # ----- T·∫£i Embedding Model (v·ªõi cache) -----
        if _CACHED_EMBEDDINGS is not None:
            print(f"\n‚ö° S·ª≠ d·ª•ng cached embedding model")
            embeddings = _CACHED_EMBEDDINGS
        else:
            print(f"\n‚è≥ ƒêang t·∫£i embedding model: {MODEL_NAME}...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"    üñ•Ô∏è S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
            
            try: # <<< Try cho vi·ªác t·∫£i embedding model >>>
                embeddings = HuggingFaceEmbeddings(
                    model_name=MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True}
                )
                _CACHED_EMBEDDINGS = embeddings  # Cache l·∫°i
                print("‚úÖ ƒê√£ t·∫£i embedding model!\n")
            except Exception as e_embed_load:
                print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG khi t·∫£i embedding model: {e_embed_load}")
                print("    Ki·ªÉm tra l·∫°i t√™n model, k·∫øt n·ªëi m·∫°ng v√† c√†i ƒë·∫∑t th∆∞ vi·ªán.")
                return None, None # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c model

        # ----- Load ho·∫∑c T·∫°o Database -----
        if os.path.exists(PERSIST_DIRECTORY):
            print(f"üìÇ Ph√°t hi·ªán Vector Store c√≥ s·∫µn t·∫°i: {PERSIST_DIRECTORY}")
            print("‚è≥ ƒêang load database...\n")
            
            try: # <<< Try cho vi·ªác load DB c√≥ s·∫µn >>>
                db = Chroma(
                    persist_directory=str(PERSIST_DIRECTORY),
                    embedding_function=embeddings
                )
                
                # Ki·ªÉm tra xem collection c√≥ d·ªØ li·ªáu kh√¥ng
                count = db._collection.count() if db._collection else 0
                
                print(f"‚úÖ ƒê√£ load Vector Store th√†nh c√¥ng!")
                print(f"    üìä S·ªë documents trong database: {count}\n")
                if count == 0:
                     print("    ‚ö†Ô∏è C·∫£nh b√°o: Database c√≥ s·∫µn nh∆∞ng kh√¥ng c√≥ document n√†o.")

            except Exception as e_db_load:
                print(f"\n‚ùå L·ªñI khi load Vector Store c√≥ s·∫µn: {e_db_load}")
                print(f"    Th·ª≠ x√≥a th∆∞ m·ª•c '{PERSIST_DIRECTORY}' v√† ch·∫°y l·∫°i ƒë·ªÉ t·∫°o m·ªõi.")
                return None, embeddings # Tr·∫£ v·ªÅ embeddings ƒë√£ load ƒë∆∞·ª£c, nh∆∞ng db l√† None
                
        else:
            print(f"üÜï Kh√¥ng t√¨m th·∫•y Vector Store. ƒêang t·∫°o m·ªõi t·ª´ {CHUNKS_FILE.name}...\n")
            
            # --- C√°c b∆∞·ªõc t·∫°o DB m·ªõi ---
            docs = None
            try: # <<< Try cho vi·ªác load v√† split file chunks >>>
                # 1. Load file chunks
                print("üìñ [1/4] ƒêang load file chunks...")
                if not CHUNKS_FILE.exists():
                     raise FileNotFoundError(f"File chunk kh√¥ng t·ªìn t·∫°i t·∫°i: {CHUNKS_FILE}")
                loader = TextLoader(str(CHUNKS_FILE), encoding='utf-8')
                documents = loader.load()
                print(f"    ‚úì ƒê√£ load {len(documents)} document base")
                
                # 2. Split documents
                print("‚úÇÔ∏è  [2/4] ƒêang split th√†nh t·ª´ng chunk v√† th√™m metadata...")
                text_splitter = RecursiveCharacterTextSplitter(
                    separators=["---"], # T√°ch theo d·∫•u ---
                    chunk_size=800,   # TƒÉng l√™n 800 ƒë·ªÉ gi·ªØ th√¥ng tin ƒë·∫ßy ƒë·ªß h∆°n
                    chunk_overlap=100,  # TƒÉng overlap ƒë·ªÉ kh√¥ng b·ªè s√≥t context
                    length_function=len
                )
                docs = text_splitter.split_documents(documents)
                if not docs:
                     print("    ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng split ƒë∆∞·ª£c chunk n√†o. Ki·ªÉm tra file v√† separator.")
                     return None, embeddings # Kh√¥ng c√≥ docs ƒë·ªÉ t·∫°o DB
                
                # TH√äM METADATA product_name cho m·ªói chunk
                for doc in docs:
                    product_name = extract_product_name(doc.page_content)
                    doc.metadata['product_name'] = product_name
                
                # TEST MODE: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng chunks n·∫øu ƒëang test
                if TEST_MODE and len(docs) > MAX_TEST_CHUNKS:
                    print(f"    ‚ö†Ô∏è TEST MODE: Ch·ªâ x·ª≠ l√Ω {MAX_TEST_CHUNKS}/{len(docs)} chunks ƒë·∫ßu ti√™n")
                    docs = docs[:MAX_TEST_CHUNKS]
                
                print(f"    ‚úì ƒê√£ split th√†nh {len(docs)} chunks v·ªõi metadata product_name")
                
            except FileNotFoundError as e_file:
                 print(f"\n‚ùå L·ªñI: {e_file}")
                 return None, embeddings
            except Exception as e_load_split:
                 print(f"\n‚ùå L·ªñI khi load ho·∫∑c split file chunks: {e_load_split}")
                 return None, embeddings

            # --- T·∫°o embeddings v√† l∆∞u ---
            try: # <<< Try cho vi·ªác t·∫°o DB m·ªõi v√† th√™m docs >>>
                print("üíæ [3/4] ƒêang t·∫°o embeddings v√† l∆∞u v√†o database...")
                print("    (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t, vui l√≤ng ƒë·ª£i...)\n")
                
                start_time = time.time()
                
                # X·ª≠ l√Ω theo batch
                batch_size = 50 # Gi·∫£m batch size n·∫øu g·∫∑p l·ªói b·ªô nh·ªõ
                total_docs = len(docs)
                
                if total_docs == 0:
                     print("    ‚ö†Ô∏è Kh√¥ng c√≥ chunk n√†o ƒë·ªÉ th√™m v√†o database.")
                     return None, embeddings # Kh√¥ng th·ªÉ t·∫°o DB r·ªóng theo c√°ch n√†y

                if total_docs <= batch_size:
                    # N·∫øu √≠t docs th√¨ t·∫°o m·ªôt l·∫ßn
                    print(f"    ‚è≥ ƒêang x·ª≠ l√Ω {total_docs} documents...")
                    db = Chroma.from_documents(
                        documents=docs,
                        embedding=embeddings,
                        persist_directory=str(PERSIST_DIRECTORY)
                    )
                    print(f"    ‚úì ƒê√£ x·ª≠ l√Ω {total_docs} documents")
                else:
                    # N·∫øu nhi·ªÅu docs th√¨ chia batch
                    print(f"    ‚è≥ ƒêang x·ª≠ l√Ω theo batch ({batch_size} docs/batch)...")
                    total_batches = (total_docs - 1) // batch_size + 1
                    
                    # Batch ƒë·∫ßu ti√™n - t·∫°o database
                    current_batch_docs = docs[:batch_size]
                    batch_start_time = time.time()
                    print(f"    ‚Üí Batch 1/{total_batches}: docs 0-{len(current_batch_docs)} ... ", end='', flush=True)
                    db = Chroma.from_documents(
                        documents=current_batch_docs,
                        embedding=embeddings,
                        persist_directory=str(PERSIST_DIRECTORY)
                    )
                    batch_time = time.time() - batch_start_time
                    print(f"‚úì ({batch_time:.1f}s)")
                    
                    # C√°c batch ti·∫øp theo - th√™m v√†o database
                    for i in range(batch_size, total_docs, batch_size):
                        batch_start = i
                        batch_end = min(i + batch_size, total_docs)
                        current_batch_docs = docs[batch_start:batch_end]
                        batch_num = (i // batch_size) + 1
                        
                        batch_start_time = time.time()
                        print(f"    ‚Üí Batch {batch_num}/{total_batches}: docs {batch_start}-{batch_end} ... ", end='', flush=True)
                        if not current_batch_docs: # Ki·ªÉm tra batch r·ªóng (d∆∞ th·ª´a nh∆∞ng an to√†n)
                             continue
                        
                        try:
                            db.add_documents(current_batch_docs)
                            batch_time = time.time() - batch_start_time
                            print(f"‚úì ({batch_time:.1f}s)")
                        except Exception as batch_error:
                            print(f"‚ùå L·ªói: {batch_error}")
                            # Ti·∫øp t·ª•c v·ªõi batch ti·∫øp theo
                        
                        # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU n·∫øu d√πng CUDA
                        if device == 'cuda':
                            torch.cuda.empty_cache()
                
                end_time = time.time()
            
                print(f"\n    ‚úì Ho√†n th√†nh sau {end_time - start_time:.2f} gi√¢y")
                # Ki·ªÉm tra l·∫°i s·ªë l∆∞·ª£ng sau khi t·∫°o
                count_after_create = db._collection.count() if db and db._collection else 0
                print(f"    üìä ƒê√£ t·∫°o v√† l∆∞u {count_after_create} vectors")
                if count_after_create != total_docs:
                     print(f"    ‚ö†Ô∏è C·∫£nh b√°o: S·ªë vector l∆∞u ({count_after_create}) kh√¥ng kh·ªõp s·ªë chunk ({total_docs}).")

                print("\n‚úÖ ƒê√£ t·∫°o Vector Store th√†nh c√¥ng!")

            except Exception as e_db_create:
                 print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG khi t·∫°o/l∆∞u Vector Store m·ªõi: {e_db_create}")
                 print(f"    Th·ª≠ ki·ªÉm tra dung l∆∞·ª£ng ·ªï ƒëƒ©a, quy·ªÅn ghi v√†o '{PERSIST_DIRECTORY}', ho·∫∑c gi·∫£m 'batch_size'.")
                 # X√≥a th∆∞ m·ª•c c√≥ th·ªÉ b·ªã t·∫°o d·ªü dang
                 if os.path.exists(PERSIST_DIRECTORY):
                      try:
                           import shutil
                           shutil.rmtree(PERSIST_DIRECTORY)
                           print(f"    ƒê√£ x√≥a th∆∞ m·ª•c '{PERSIST_DIRECTORY}' c√≥ th·ªÉ b·ªã l·ªói.")
                      except Exception as e_del:
                           print(f"    Kh√¥ng th·ªÉ x√≥a th∆∞ m·ª•c l·ªói '{PERSIST_DIRECTORY}': {e_del}")
                 db = None # ƒê·∫∑t l·∫°i db th√†nh None v√¨ t·∫°o l·ªói
                 return None, embeddings # Tr·∫£ v·ªÅ embeddings nh∆∞ng db l√† None

    except Exception as e_global: 
         print(f"\n‚ùå ƒê√É X·∫¢Y RA L·ªñI KH√îNG X√ÅC ƒê·ªäNH: {e_global}")
         return None, None 

    return db, embeddings

def setup_rag_chain(db):
    """Thi·∫øt l·∫≠p RAG chain v·ªõi Retriever, LLM v√† Prompt"""
    print("\n" + "=" * 80)
    print("‚õìÔ∏è KH·ªûI T·∫†O RAG CHAIN")
    print("=" * 80)
    
    # Check if db is None
    if db is None:
        print("\n‚ùå L·ªñI: Vector store ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
        return None
    
    # 1. Kh·ªüi t·∫°o LLM (t·ªëi ∆∞u cho 2-3 s·∫£n ph·∫©m ƒê·ªíNG NH·∫§T)
    print("\nü§ñ [1/3] ƒêang k·∫øt n·ªëi v·ªõi Google Gemini...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", 
        temperature=0.3,  
        max_output_tokens=2000,  
        convert_system_message_to_human=True,
        request_timeout=90,
        max_retries=3  
    )
    print("    ‚úì ƒê√£ k·∫øt n·ªëi Gemini 2.5 Flash (t·ªëi ∆∞u cho server: 2-3 s·∫£n ph·∫©m ƒê·ªíNG NH·∫§T)")
    
    # 2. T·∫°o Retriever (tƒÉng kh·∫£ nƒÉng t√¨m ki·∫øm)
    print("üîç [2/3] ƒêang t·∫°o Retriever...")
    retriever = db.as_retriever(
        search_type="similarity",  
        search_kwargs={
            "k": 30  # TƒÉng l√™n 30 ƒë·ªÉ ƒë·∫£m b·∫£o t√¨m ƒë·ªß chunks cho nhi·ªÅu s·∫£n ph·∫©m
        }
    )
    print("    ‚úì Retriever: t√¨m 30 chunks relevant nh·∫•t (similarity search)")
    
    # 3. T·∫°o Prompt Template
    print("üìù [3/3] ƒêang t·∫°o Prompt Template...")
    template = """You are a strict assistant. You must answer questions based ONLY on the provided context below. DO NOT use your internal knowledge to update or guess prices. If the price is not mentioned in the context, say 'Price not available'.

B·∫°n l√† chuy√™n gia t∆∞ v·∫•n m·ªπ ph·∫©m chuy√™n nghi·ªáp, th√¢n thi·ªán v√† hi·ªÉu t√¢m l√Ω kh√°ch h√†ng, KH√îNG PH·∫¢I B√ÅC Sƒ®.

- M·ª•c ti√™u: Gi√∫p ng∆∞·ªùi d√πng t√¨m s·∫£n ph·∫©m l√†m s·∫°ch, d∆∞·ª°ng ·∫©m, b·∫£o v·ªá da ph√π h·ª£p v·ªõi t√¨nh tr·∫°ng c·ªßa h·ªç.
- Kh√¥ng d√πng c√°c t·ª´ kh·∫≥ng ƒë·ªãnh ch·ªØa b·ªánh nh∆∞: "tr·ªã d·ª©t ƒëi·ªÉm", "thu·ªëc", "cam k·∫øt kh·ªèi".
- H√£y d√πng c√°c t·ª´: "h·ªó tr·ª£", "l√†m d·ªãu", "c·∫£i thi·ªán", "gi√∫p da kh·ªèe h∆°n".

PH√ÇN LO·∫†I C√ÇU H·ªéI V√Ä C√ÅCH TR·∫¢ L·ªúI:

üîπ **CH√ÄO H·ªéI/GIAO TI·∫æP C∆† B·∫¢N**
C√¢u h·ªèi: "xin ch√†o", "hi", "hello", "ch√†o b·∫°n", "hey"
‚Üí "Ch√†o b·∫°n! üëã M√¨nh l√† tr·ª£ l√Ω t∆∞ v·∫•n m·ªπ ph·∫©m. B·∫°n mu·ªën t√¨m s·∫£n ph·∫©m g√¨ h√¥m nay? üòä"

üîπ **H·ªéI V·ªÄ CH·ª®C NƒÇNG/GI·ªöI THI·ªÜU**
C√¢u h·ªèi: "b·∫°n l√† ai", "b·∫°n l√†m g√¨", "c√≥ th·ªÉ gi√∫p g√¨", "b·∫°n bi·∫øt g√¨"
‚Üí "M√¨nh l√† chuy√™n gia t∆∞ v·∫•n m·ªπ ph·∫©m! üíÑ M√¨nh c√≥ th·ªÉ gi√∫p b·∫°n:
‚Ä¢ T√¨m s·∫£n ph·∫©m theo lo·∫°i da (kh√¥, d·∫ßu, nh·∫°y c·∫£m, h·ªón h·ª£p, th∆∞·ªùng)
‚Ä¢ T∆∞ v·∫•n s·∫£n ph·∫©m theo B·ªÜNH DA (m·ª•n, ch√†m, v·∫£y n·∫øn, tr·ª©ng c√° ƒë·ªè, n·∫•m da...)
‚Ä¢ T∆∞ v·∫•n kem d∆∞·ª°ng, serum, toner, m·∫∑t n·∫°, s·ªØa r·ª≠a m·∫∑t, kem ch·ªëng n·∫Øng
‚Ä¢ Gi·∫£i th√≠ch th√†nh ph·∫ßn v√† c√¥ng d·ª•ng s·∫£n ph·∫©m
‚Ä¢ G·ª£i √Ω routine chƒÉm s√≥c da
B·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ g√¨ v·ªÅ da ho·∫∑c c·∫ßn t√¨m s·∫£n ph·∫©m n√†o? üòä"

üîπ **H·ªéI CHUNG CHUNG KH√îNG C·ª§ TH·ªÇ**
C√¢u h·ªèi: "c√≥ s·∫£n ph·∫©m g√¨", "cho xem s·∫£n ph·∫©m", "g·ª£i √Ω s·∫£n ph·∫©m", "b·∫°n c√≥ th·ªÉ cho m·∫•y s·∫£n ph·∫©m"
‚Üí "M√¨nh c√≥ r·∫•t nhi·ªÅu s·∫£n ph·∫©m! üòä ƒê·ªÉ t∆∞ v·∫•n ch√≠nh x√°c, b·∫°n cho m√¨nh bi·∫øt:
‚Ä¢ Lo·∫°i da c·ªßa b·∫°n? (kh√¥/d·∫ßu/h·ªón h·ª£p/nh·∫°y c·∫£m/th∆∞·ªùng)
‚Ä¢ B·ªánh da (n·∫øu c√≥)? (m·ª•n/ch√†m/v·∫£y n·∫øn/tr·ª©ng c√° ƒë·ªè/n·∫•m da...)
‚Ä¢ Lo·∫°i s·∫£n ph·∫©m c·∫ßn? (kem d∆∞·ª°ng/serum/toner/m·∫∑t n·∫°/s·ªØa r·ª≠a m·∫∑t...)
‚Ä¢ Ho·∫∑c v·∫•n ƒë·ªÅ da b·∫°n mu·ªën gi·∫£i quy·∫øt? (m·ª•n/th√¢m/l√£o h√≥a/d∆∞·ª°ng ·∫©m...)
Cho m√¨nh bi·∫øt ƒë·ªÉ m√¨nh t∆∞ v·∫•n ƒë√∫ng nhu c·∫ßu nh√©! üíï"

üîπ **H·ªéI V·ªÄ B·ªÜNH DA (∆ØU TI√äN CAO)**
C√¢u h·ªèi: "t√¥i b·ªã m·ª•n", "t√¥i b·ªã ch√†m", "da b·ªã v·∫£y n·∫øn", "b·ªã tr·ª©ng c√° ƒë·ªè", "n·∫•m da", "m·ª•n c√≥c"...

‚ö†Ô∏è **KI·ªÇM TRA PH·∫†M VI T∆Ø V·∫§N:**
‚Üí CH·ªà t∆∞ v·∫•n cho C√ÅC B·ªÜNH DA SAU (c√≥ trong database):
   ‚Ä¢ M·ª•n (Acne) ‚Üí H·ªón h·ª£p/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ Ch√†m (Eczema) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ V·∫£y n·∫øn (Psoriasis) ‚Üí Kh√¥
   ‚Ä¢ Tr·ª©ng c√° ƒë·ªè (Rosacea) ‚Üí H·ªón h·ª£p/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ D√†y s·ª´ng (Actinic Keratosis) ‚Üí Kh√¥/Th∆∞·ªùng
   ‚Ä¢ N·∫•m da (Tinea) ‚Üí H·ªón h·ª£p/D·∫ßu
   ‚Ä¢ Vi√™m da ti·∫øt b√£ (Seborrheic Keratoses) ‚Üí Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ T·ªïn th∆∞∆°ng n·∫Øng (Sun Damage) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/Nh·∫°y c·∫£m
   ‚Ä¢ M·ª•n c√≥c (Warts) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ Ph√°t ban thu·ªëc (Drug Eruption) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m

‚Üí **N·∫æU B·ªÜNH DA KH√îNG TRONG DANH S√ÅCH TR√äN** (vd: gh·∫ª, lang ben, zona, herpes, vi√™m da c∆° ƒë·ªãa n·∫∑ng...):
   "‚ö†Ô∏è Xin l·ªói, b·ªánh [t√™n b·ªánh] N·∫∞M NGO√ÄI PH·∫†M VI t∆∞ v·∫•n m·ªπ ph·∫©m c·ªßa m√¨nh.
   
   üè• KHUY·∫æN C√ÅO:
   ‚Ä¢ ƒê√¢y l√† b·ªánh da C·∫¶N ƒêI·ªÄU TR·ªä Y KHOA, kh√¥ng n√™n t·ª± chƒÉm s√≥c b·∫±ng m·ªπ ph·∫©m
   ‚Ä¢ Vui l√≤ng ƒê·∫∂T L·ªäCH G·∫∂P B√ÅC Sƒ® DA LI·ªÑU ƒë·ªÉ ƒë∆∞·ª£c kh√°m v√† k√™ ƒë∆°n thu·ªëc ph√π h·ª£p
   ‚Ä¢ M·ªπ ph·∫©m ch·ªâ h·ªó tr·ª£ sau khi ƒë√£ ƒë∆∞·ª£c b√°c sƒ© ƒëi·ªÅu tr·ªã
   
   üí° M√¨nh c√≥ th·ªÉ t∆∞ v·∫•n m·ªπ ph·∫©m cho c√°c v·∫•n ƒë·ªÅ da th√¥ng th∆∞·ªùng nh∆∞: m·ª•n, ch√†m, v·∫£y n·∫øn, tr·ª©ng c√° ƒë·ªè, n·∫•m da... B·∫°n c√≥ v·∫•n ƒë·ªÅ da n√†o trong s·ªë n√†y kh√¥ng?"

‚Üí **N·∫æU B·ªÜNH DA C√ì TRONG DANH S√ÅCH - TR·∫¢ L·ªúI NG·∫ÆN G·ªåN:**
   "D·∫°, m√¨nh g·ª£i √Ω s·∫£n ph·∫©m cho [t√™n b·ªánh TRONG C√ÇU H·ªéI HI·ªÜN T·∫†I] nh√©:
   
   [LI·ªÜT K√ä 2-3 S·∫¢N PH·∫®M NGAY - KH√îNG D√ÄI D√íNG]"
   
   ‚ö†Ô∏è QUAN TR·ªåNG: 
   ‚Ä¢ CH·ªà tr·∫£ l·ªùi v·ªÅ b·ªánh da ƒë∆∞·ª£c N√äU TRONG C√ÇU H·ªéI HI·ªÜN T·∫†I
   ‚Ä¢ KH√îNG ƒë∆∞·ª£c nh·∫Øc l·∫°i ho·∫∑c nh·∫ßm l·∫´n v·ªõi c√°c b·ªánh da ƒë∆∞·ª£c h·ªèi ·ªü c√¢u h·ªèi tr∆∞·ªõc
   ‚Ä¢ KH√îNG c·∫ßn gi·∫£i th√≠ch d√†i d√≤ng, ƒêI TH·∫≤NG V√ÄO S·∫¢N PH·∫®M

üîπ **H·ªéI V·ªÄ V·∫§N ƒê·ªÄ DA (KH√îNG PH·∫¢I B·ªÜNH)**
C√¢u h·ªèi: "da t√¥i kh√¥", "da d·∫ßu nhi·ªÅu", "da nh·∫°y c·∫£m", "da h·ªón h·ª£p"
‚Üí ƒêI TH·∫≤NG V√ÄO: "D·∫°, m√¨nh g·ª£i √Ω s·∫£n ph·∫©m cho da [lo·∫°i da] nh√©:
   [LI·ªÜT K√ä 2-3 S·∫¢N PH·∫®M NGAY]"

üîπ **H·ªéI THEO LO·∫†I S·∫¢N PH·∫®M**
C√¢u h·ªèi: "c√≥ kem d∆∞·ª°ng n√†o...", "serum g√¨ t·ªët", "toner cho da...", "m·∫∑t n·∫°..."
‚Üí ƒêI TH·∫≤NG V√ÄO S·∫¢N PH·∫®M, m·∫∑c ƒë·ªãnh 2-3 s·∫£n ph·∫©m
‚Üí N·∫øu user Y√äU C·∫¶U S·ªê L∆Ø·ª¢NG C·ª§ TH·ªÇ (vd: "cho t√¥i 3 s·∫£n ph·∫©m", "4 s·∫£n ph·∫©m"):
   ‚Ä¢ TR·∫¢ V·ªÄ ƒê√öNG S·ªê ƒê√ì (t·ªëi ƒëa 4)
   ‚Ä¢ N·∫øu "nhi·ªÅu s·∫£n ph·∫©m" ‚Üí tr·∫£ v·ªÅ 4 s·∫£n ph·∫©m

üîπ **H·ªéI V·ªÄ TH∆Ø∆†NG HI·ªÜU**
C√¢u h·ªèi: "b·∫°n c√≥ [t√™n th∆∞∆°ng hi·ªáu] kh√¥ng", "s·∫£n ph·∫©m c·ªßa [brand]"
‚Üí Ki·ªÉm tra database, n·∫øu c√≥ th√¨ li·ªát k√™, n·∫øu kh√¥ng: "M√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ [brand] trong database. B·∫°n mu·ªën t∆∞ v·∫•n s·∫£n ph·∫©m theo lo·∫°i da hay v·∫•n ƒë·ªÅ c·ª• th·ªÉ kh√¥ng? üîç"

üîπ **H·ªéI SO S√ÅNH**
C√¢u h·ªèi: "A hay B t·ªët h∆°n", "kh√°c nhau th·∫ø n√†o", "n√™n ch·ªçn c√°i n√†o"
‚Üí So s√°nh 2 s·∫£n ph·∫©m d·ª±a tr√™n TH√ÄNH PH·∫¶N, C√îNG D·ª§NG, LO·∫†I DA ph√π h·ª£p

üîπ **H·ªéI GI√Å/MUA ·ªû ƒê√ÇU**
C√¢u h·ªèi: "gi√° bao nhi√™u", "mua ·ªü ƒë√¢u", "c√≥ ship kh√¥ng"
‚Üí "Xin l·ªói, m√¨nh ch·ªâ t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m th√¥i nh√©. B·∫°n c√≥ th·ªÉ mua t·∫°i c√°c store ch√≠nh h√£ng ho·∫∑c website c·ªßa th∆∞∆°ng hi·ªáu. M√¨nh c√≥ th·ªÉ t∆∞ v·∫•n th√™m v·ªÅ s·∫£n ph·∫©m kh√°c kh√¥ng? üòä"

üîπ **H·ªéI ROUTINE/C√ÅCH D√ôNG**
C√¢u h·ªèi: "routine cho da...", "th·ª© t·ª± d√πng", "d√πng nh∆∞ th·∫ø n√†o", "d√πng khi n√†o"
‚Üí G·ª£i √Ω routine c∆° b·∫£n: S√°ng (s·ªØa r·ª≠a m·∫∑t ‚Üí toner ‚Üí serum ‚Üí kem d∆∞·ª°ng ‚Üí ch·ªëng n·∫Øng), T·ªëi (t∆∞∆°ng t·ª± nh∆∞ng thay ch·ªëng n·∫Øng = m·∫∑t n·∫° 2-3 l·∫ßn/tu·∫ßn)

üîπ **C·∫¢M ∆†N/T·∫†M BI·ªÜT**
C√¢u h·ªèi: "c·∫£m ∆°n", "thank you", "ok r·ªìi", "t·∫°m bi·ªát", "bye"
‚Üí "Kh√¥ng c√≥ g√¨! üòä Ch√∫c b·∫°n c√≥ l√†n da ƒë·∫πp! H·∫πn g·∫∑p l·∫°i b·∫°n! üíï"

üîπ **C√ÇU H·ªéI NGO√ÄI L·ªÄ**
C√¢u h·ªèi: th·ªùi ti·∫øt, tin t·ª©c, th·ªÉ thao, ch√≠nh tr·ªã, to√°n h·ªçc...
‚Üí "Xin l·ªói, m√¨nh ch·ªâ chuy√™n v·ªÅ m·ªπ ph·∫©m v√† skincare th√¥i üíÑ B·∫°n c√≥ mu·ªën h·ªèi v·ªÅ chƒÉm s√≥c da kh√¥ng?"

---

**CH√ö √ù KHI TR·∫¢ L·ªúI:**
- Lu√¥n TH√ÇN THI·ªÜN, d√πng "m√¨nh/b·∫°n" thay v√¨ "t√¥i/anh/ch·ªã"
- **NG·∫ÆN G·ªåN - ƒêI TH·∫≤NG V√ÄO S·∫¢N PH·∫®M:**
  ‚Ä¢ KH√îNG gi·∫£i th√≠ch d√†i d√≤ng v·ªÅ b·ªánh da hay v·∫•n ƒë·ªÅ da
  ‚Ä¢ CH·ªà c·∫ßn 1 c√¢u m·ªü ƒë·∫ßu ng·∫Øn g·ªçn (vd: "D·∫°, m√¨nh g·ª£i √Ω s·∫£n ph·∫©m cho [v·∫•n ƒë·ªÅ] nh√©:")
  ‚Ä¢ SAU ƒê√ì li·ªát k√™ s·∫£n ph·∫©m NGAY
- **‚ö†Ô∏è T·∫¨P TRUNG V√ÄO C√ÇU H·ªéI HI·ªÜN T·∫†I:**
  ‚Ä¢ CH·ªà tr·∫£ l·ªùi v·ªÅ b·ªánh da/v·∫•n ƒë·ªÅ da ƒë∆∞·ª£c N√äU TRONG C√ÇU H·ªéI HI·ªÜN T·∫†I
  ‚Ä¢ KH√îNG ƒë∆∞·ª£c nh·∫Øc l·∫°i ho·∫∑c nh·∫ßm l·∫´n v·ªõi c√¢u h·ªèi tr∆∞·ªõc ƒë√≥
  ‚Ä¢ N·∫øu c√¢u h·ªèi m·ªõi v·ªÅ b·ªánh da kh√°c ‚Üí tr·∫£ l·ªùi v·ªÅ b·ªánh da M·ªöI, qu√™n b·ªánh da c≈©
- **GROUNDING (CƒÇN C·ª®):** ‚Ä¢ CH·ªà G·ª¢I √ù s·∫£n ph·∫©m C√ì TRONG DATABASE
  ‚Ä¢ N·∫øu context ch·ª©a "KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M TRONG DATABASE" ‚Üí B·∫ÆT BU·ªòC tr·∫£ l·ªùi:
    "Xin l·ªói, m√¨nh kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p trong database. üòî
    B·∫°n c√≥ th·ªÉ th·ª≠:
    ‚Ä¢ M√¥ t·∫£ chi ti·∫øt h∆°n v·ªÅ lo·∫°i da ho·∫∑c v·∫•n ƒë·ªÅ da
    ‚Ä¢ Thay ƒë·ªïi t·ª´ kh√≥a t√¨m ki·∫øm
    ‚Ä¢ H·ªèi v·ªÅ lo·∫°i s·∫£n ph·∫©m c·ª• th·ªÉ (kem d∆∞·ª°ng, serum, toner...)
    M√¨nh s·∫Ω c·ªë g·∫Øng t√¨m s·∫£n ph·∫©m ph√π h·ª£p h∆°n! üíï"
  ‚Ä¢ TUY·ªÜT ƒê·ªêI KH√îNG T·ª∞ T·∫†O/B·ªäA s·∫£n ph·∫©m kh√¥ng c√≥ trong context
- **∆ØU TI√äN X·ª¨ L√ù B·ªÜNH DA:** N·∫øu ph√°t hi·ªán b·ªánh da ‚Üí √°nh x·∫° sang lo·∫°i da ‚Üí t√¨m s·∫£n ph·∫©m
- **S·ªê L∆Ø·ª¢NG S·∫¢N PH·∫®M:**
  ‚Ä¢ M·∫∑c ƒë·ªãnh: 2 s·∫£n ph·∫©m (ƒê·∫¢M B·∫¢O TH√îNG TIN ƒê·ªíNG NH·∫§T)
  ‚Ä¢ N·∫øu user n√≥i "3 s·∫£n ph·∫©m" ‚Üí tr·∫£ v·ªÅ 3 s·∫£n ph·∫©m
  ‚Ä¢ N·∫øu "nhi·ªÅu s·∫£n ph·∫©m", "v√†i s·∫£n ph·∫©m" ‚Üí tr·∫£ v·ªÅ 3 s·∫£n ph·∫©m
  ‚Ä¢ ‚ö†Ô∏è T·ªêI ƒêA 3 S·∫¢N PH·∫®M ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng
- **‚ö†Ô∏è FORMAT M·ªñI S·∫¢N PH·∫®M (B·∫ÆT BU·ªòC ƒêI·ªÄN ƒê·∫¶Y ƒê·ª¶):**
  **S·ªë. T√™n s·∫£n ph·∫©m c·ªßa TH∆Ø∆†NG HI·ªÜU** Gi√°: XXX.XXX VND | ƒê√°nh gi√°: X/5 | Lo·∫°i da: [...]
  
  ‚ö†Ô∏è QUAN TR·ªåNG: 
  ‚Ä¢ B·∫ÆT BU·ªòC ph·∫£i c√≥ ƒê·∫¶Y ƒê·ª¶: T√™n, Th∆∞∆°ng hi·ªáu, Gi√°, ƒê√°nh gi√°, Lo·∫°i da
  ‚Ä¢ KH√îNG hi·ªÉn th·ªã c√¥ng d·ª•ng ho·∫∑c th√†nh ph·∫ßn (ch·ªâ th√¥ng tin c∆° b·∫£n)
  ‚Ä¢ N·∫æU thi·∫øu th√¥ng tin trong context ‚Üí ghi "(Kh√¥ng c√≥ th√¥ng tin)"
  ‚Ä¢ T·∫§T C·∫¢ s·∫£n ph·∫©m ph·∫£i c√≥ FORMAT GI·ªêNG NHAU
- **GI√Å ƒê√É ƒê∆Ø·ª¢C CHUY·ªÇN ƒê·ªîI** sang VND trong context, CH·ªà HI·ªÇN TH·ªä VND, KH√îNG HI·ªÇN TH·ªä USD
- **LO·∫†I DA PH·∫¢I D·ªäCH SANG TI·∫æNG VI·ªÜT:**
  ‚Ä¢ Combination ‚Üí H·ªón h·ª£p
  ‚Ä¢ Dry ‚Üí Kh√¥
  ‚Ä¢ Normal ‚Üí Th∆∞·ªùng
  ‚Ä¢ Oily ‚Üí D·∫ßu
  ‚Ä¢ Sensitive ‚Üí Nh·∫°y c·∫£m
  V√≠ d·ª•: "Lo·∫°i da: H·ªón h·ª£p, Kh√¥, Th∆∞·ªùng" (KH√îNG ƒë∆∞·ª£c ƒë·ªÉ "Combination, Dry, Normal")
- D√πng emoji ph√π h·ª£p: üòäüíÑ‚ú®üíïüëãüíäü©∫
- N·∫øu KH√îNG ch·∫Øc ch·∫Øn: "B·∫°n c√≥ th·ªÉ m√¥ t·∫£ c·ª• th·ªÉ h∆°n v·ªÅ [v·∫•n ƒë·ªÅ] ƒë·ªÉ m√¨nh t∆∞ v·∫•n ch√≠nh x√°c h∆°n kh√¥ng?"

TH√îNG TIN S·∫¢N PH·∫®M:
{context}

C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI (m·∫∑c ƒë·ªãnh 2 s·∫£n ph·∫©m, ho·∫∑c ƒë√∫ng s·ªë l∆∞·ª£ng user y√™u c·∫ßu n·∫øu c√≥):"""
    
    prompt = ChatPromptTemplate.from_template(template)
    print("    ‚úì ƒê√£ t·∫°o Prompt Template (compact + smart filtering)")
    
    # 4. X√¢y d·ª±ng RAG Chain v·ªõi NH√ìM CHUNKS THEO S·∫¢N PH·∫®M v√† GROUNDING CHECK
    def format_docs(docs):
        """Format documents: NH√ìM chunks theo product_name, l·∫•y 3-4 s·∫£n ph·∫©m, s·∫Øp x·∫øp chunks theo lo·∫°i"""
        
        # GROUNDING CHECK: Ki·ªÉm tra xem c√≥ chunks kh√¥ng
        if not docs or len(docs) == 0:
            return "KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M TRONG DATABASE"
        
        # DEBUG: In s·ªë chunks t√¨m ƒë∆∞·ª£c
        print(f"    üîç T√¨m ƒë∆∞·ª£c {len(docs)} chunks t·ª´ database")
        
        # B∆∞·ªõc 1: Nh√≥m c√°c chunks theo product_name v√† theo d√µi th·ª© t·ª± xu·∫•t hi·ªán
        product_groups = {}  # {product_name: {'chunks': [], 'first_index': int, 'metadata': {}}}
        
        for idx, doc in enumerate(docs):
            product_name = doc.metadata.get('product_name', 'Unknown Product')
            
            if product_name not in product_groups:
                # Tr√≠ch xu·∫•t metadata t·ª´ chunk ƒë·∫ßu ti√™n
                content_lower = doc.page_content.lower()
                metadata = {
                    'brand': extract_field_from_chunk(doc.page_content, 'Brand'),
                    'category': extract_field_from_chunk(doc.page_content, 'Category'),
                    'suitable_for': extract_field_from_chunk(doc.page_content, 'Suitable for'),
                    'rank': extract_field_from_chunk(doc.page_content, 'Rank'),
                    'price': extract_field_from_chunk(doc.page_content, 'Price')
                }
                
                product_groups[product_name] = {
                    'chunks': [],
                    'first_index': idx,  # L∆∞u v·ªã tr√≠ xu·∫•t hi·ªán ƒë·∫ßu ti√™n (relevance score)
                    'metadata': metadata,
                    'has_summary': False,
                    'has_ingredients': False
                }
            
            # ƒê√°nh d·∫•u lo·∫°i chunk
            if 'chunk type: product summary' in doc.page_content.lower():
                product_groups[product_name]['has_summary'] = True
            if 'chunk type: ingredients' in doc.page_content.lower():
                product_groups[product_name]['has_ingredients'] = True
            
            product_groups[product_name]['chunks'].append(doc)
        
        # GROUNDING CHECK: Ki·ªÉm tra c√≥ s·∫£n ph·∫©m n√†o kh√¥ng
        if not product_groups or len(product_groups) == 0:
            return "KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M TRONG DATABASE"
        
        # DEBUG: In s·ªë s·∫£n ph·∫©m t√¨m ƒë∆∞·ª£c
        print(f"    üì¶ T√¨m ƒë∆∞·ª£c {len(product_groups)} s·∫£n ph·∫©m kh√°c nhau")
        
        # B∆∞·ªõc 2: L·ªçc s·∫£n ph·∫©m c√≥ ƒë·ªß th√¥ng tin (∆∞u ti√™n c√≥ summary)
        complete_products = []
        for name, data in product_groups.items():
            if data['has_summary']:  # ∆Øu ti√™n s·∫£n ph·∫©m c√≥ summary
                complete_products.append((name, data))
        
        # N·∫øu kh√¥ng c√≥ s·∫£n ph·∫©m n√†o c√≥ summary, l·∫•y t·∫•t c·∫£
        if not complete_products:
            complete_products = list(product_groups.items())
        
        # B∆∞·ªõc 3: S·∫Øp x·∫øp s·∫£n ph·∫©m theo relevance (first_index c√†ng nh·ªè = c√†ng relevant)
        sorted_products = sorted(
            complete_products,
            key=lambda x: x[1]['first_index']
        )
        
        # B∆∞·ªõc 4: Ch·ªçn top 3 s·∫£n ph·∫©m ƒë·ªÉ ƒë·∫£m b·∫£o ƒê·ªíNG NH·∫§T th√¥ng tin
        num_products = min(3, len(sorted_products))  # T·ªëi ƒëa 3 s·∫£n ph·∫©m
        selected_products = sorted_products[:num_products]
        
        print(f"    ‚úÖ Ch·ªçn {num_products} s·∫£n ph·∫©m ƒë·ªÉ t∆∞ v·∫•n")
        
        # B∆∞·ªõc 5: G·ªôp v√† format chunks c·ªßa m·ªói s·∫£n ph·∫©m
        formatted = []
        for i, (product_name, data) in enumerate(selected_products, 1):
            chunks = data['chunks']
            metadata = data['metadata']
            
            # Lo·∫°i b·ªè duplicate chunks (d·ª±a tr√™n page_content)
            seen_contents = set()
            unique_chunks = []
            for chunk in chunks:
                content_hash = hash(chunk.page_content.strip())
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    unique_chunks.append(chunk)
            
            # S·∫Øp x·∫øp chunks theo lo·∫°i: Summary tr∆∞·ªõc, Ingredients sau
            def chunk_priority(chunk):
                content = chunk.page_content.lower()
                if 'chunk type: product summary' in content:
                    return 0  # Summary ƒë·∫ßu ti√™n
                elif 'chunk type: ingredients' in content:
                    return 1  # Ingredients sau
                else:
                    return 2  # C√°c lo·∫°i kh√°c cu·ªëi c√πng
            
            sorted_chunks = sorted(unique_chunks, key=chunk_priority)
            
            # G·ªôp th√¥ng tin s·∫£n ph·∫©m v·ªõi header r√µ r√†ng
            product_info = f"{'='*80}\n"
            product_info += f"S·∫¢N PH·∫®M #{i}: {product_name}\n"
            product_info += f"{'='*80}\n"
            
            # Th√™m metadata t·ªïng h·ª£p n·∫øu c√≥
            if metadata['brand']:
                product_info += f"üè¢ Th∆∞∆°ng hi·ªáu: {metadata['brand']}\n"
            if metadata['category']:
                product_info += f"üìÅ Lo·∫°i: {metadata['category']}\n"
            if metadata['suitable_for']:
                product_info += f"üë§ Ph√π h·ª£p: {metadata['suitable_for']}\n"
            if metadata['rank']:
                product_info += f"‚≠ê ƒê√°nh gi√°: {metadata['rank']}\n"
            if metadata['price']:
                price_vnd = convert_price_in_text(f"Price: {metadata['price']}")
                product_info += f"üí∞ {price_vnd}\n"
            
            product_info += f"{'-'*80}\n\n"
            
            # Th√™m n·ªôi dung chi ti·∫øt t·ª´ chunks
            for chunk in sorted_chunks:
                content = chunk.page_content.strip()
                # Chuy·ªÉn ƒë·ªïi gi√° USD ‚Üí VND
                content = convert_price_in_text(content)
                product_info += content + "\n\n"
            
            formatted.append(product_info)
        
        result = "\n\n".join(formatted)
        
        return result
    
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("\n‚úÖ RAG Chain ƒë√£ s·∫µn s√†ng!")
    print("\nüìä Lu·ªìng ho·∫°t ƒë·ªông (C·∫¢I TI·∫æN):")
    print("    1Ô∏è‚É£  User Question ‚Üí Retriever")
    print("    2Ô∏è‚É£  Retriever ‚Üí 30 chunks (similarity search)")
    print("    3Ô∏è‚É£  Tr√≠ch xu·∫•t metadata t·ª´ chunks")
    print("    4Ô∏è‚É£  NH√ìM theo product_name + Filter s·∫£n ph·∫©m c√≥ ƒë·ªß th√¥ng tin")
    print("    5Ô∏è‚É£  S·∫Øp x·∫øp theo relevance ‚Üí Ch·ªçn top 3 s·∫£n ph·∫©m")
    print("    6Ô∏è‚É£  Lo·∫°i b·ªè duplicate + S·∫Øp x·∫øp: Summary ‚Üí Ingredients")
    print("    7Ô∏è‚É£  Format structured v·ªõi metadata r√µ r√†ng")
    print("    8Ô∏è‚É£  Context + Question ‚Üí LLM ‚Üí 3 s·∫£n ph·∫©m CH√çNH X√ÅC & ƒê·∫¶Y ƒê·ª¶ ‚ö°")
    print("    ‚ö†Ô∏è  C·∫£i ti·∫øn: Metadata extraction + Structured format + Better filtering")

    return rag_chain

# =============================================================================
# CHAT HISTORY
# =============================================================================
def save_chat_history(chat_history):
    """L∆∞u l·ªãch s·ª≠ chat v√†o file JSON"""
    try:
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        CHAT_HISTORY_DIR.mkdir(exist_ok=True)
        
        # T√™n file theo timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = CHAT_HISTORY_DIR / f"chat_{timestamp}.json"
        
        # L∆∞u v√†o file
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chat_history, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ ƒê√£ l∆∞u l·ªãch s·ª≠ chat: {filename.name}")
        return filename
    except Exception as e:
        print(f"\n‚ö†Ô∏è  L·ªói khi l∆∞u l·ªãch s·ª≠: {str(e)}")
        return None

def load_latest_chat_history():
    """Load l·ªãch s·ª≠ chat g·∫ßn nh·∫•t (n·∫øu c√≥)"""
    try:
        if not CHAT_HISTORY_DIR.exists():
            return None
        
        # T√¨m file m·ªõi nh·∫•t
        chat_files = list(CHAT_HISTORY_DIR.glob("chat_*.json"))
        if not chat_files:
            return None
        
        latest_file = max(chat_files, key=lambda f: f.stat().st_mtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            history = json.load(f)
        
        return history, latest_file
    except Exception as e:
        print(f"\n‚ö†Ô∏è  L·ªói khi load l·ªãch s·ª≠: {str(e)}")
        return None

# =============================================================================
# VISION ANALYSIS (MERGED: NEW LOGIC + OLD BACKEND SUPPORT)
# =============================================================================
def analyze_skin_image(image_input, note: str = None):
    """
    Ph√¢n t√≠ch ·∫£nh da b·∫±ng VLM - T·∫≠p trung v√†o m·ª©c ƒë·ªô nghi√™m tr·ªçng
    Supports: File Path (CLI) and Base64/Bytes (Backend)
    """
    try:
        print("\nüì∏ ƒêang ph√¢n t√≠ch t√¨nh tr·∫°ng da t·ª´ ·∫£nh...")
        
        img = None
        # X·ª≠ l√Ω input ƒëa d·∫°ng (Merge t·ª´ file c≈©)
        if isinstance(image_input, str):
            # Check for data URI or base64 string
            if image_input.startswith('data:image'):
                image_input = image_input.split(',')[1]
                image_bytes = base64.b64decode(image_input)
                img = Image.open(io.BytesIO(image_bytes))
            elif os.path.exists(image_input):
                 # L√† ƒë∆∞·ªùng d·∫´n file
                img = Image.open(image_input)
            else:
                # Th·ª≠ decode base64 thu·∫ßn
                try:
                    image_bytes = base64.b64decode(image_input)
                    img = Image.open(io.BytesIO(image_bytes))
                except:
                     print(f"‚ùå Input string kh√¥ng ph·∫£i l√† path h·ª£p l·ªá hay base64.")
                     return None
        elif isinstance(image_input, bytes):
            img = Image.open(io.BytesIO(image_input))
        elif isinstance(image_input, Image.Image):
             img = image_input
        
        if img is None:
             print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c ·∫£nh t·ª´ input.")
             return None

        # Kh·ªüi t·∫°o Gemini Vision model (T·ª´ file m·ªõi)
        vision_model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Prompt t·∫≠p trung v√†o m·ª©c ƒë·ªô nghi√™m tr·ªçng (T·ª´ file m·ªõi)
        vision_prompt = """B·∫°n l√† chuy√™n gia da li·ªÖu. Ph√¢n t√≠ch ·∫£nh da v√† T√ìM T·∫ÆT NG·∫ÆN G·ªåN:

1. LO·∫†I DA: (kh√¥/d·∫ßu/h·ªón h·ª£p/nh·∫°y c·∫£m/th∆∞·ªùng)

2. V·∫§N ƒê·ªÄ CH√çNH & M·ª®C ƒê·ªò NGHI√äM TR·ªåNG:
- N·∫øu c√≥ m·ª•n: lo·∫°i m·ª•n (vi√™m/ƒë·∫ßu ƒëen/ƒë·∫ßu tr·∫Øng/b·ªçc), m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG)
- N·∫øu c√≥ th√¢m/s·∫πo: m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG), m√†u s·∫Øc, ph√¢n b·ªë
- N·∫øu c√≥ l√£o h√≥a: m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG)
- N·∫øu c√≥ v·∫•n ƒë·ªÅ kh√°c: n√™u r√µ

3. M·ª®C ƒê·ªò CHUNG: Ch·ªçn 1 trong 4:
   - NH·∫∏: V·∫•n ƒë·ªÅ nh·ªè, √≠t n·ªët, c√≥ th·ªÉ t·ª± chƒÉm s√≥c
   - TRUNG B√åNH: V·∫•n ƒë·ªÅ r√µ r√†ng, nhi·ªÅu n·ªët, c·∫ßn s·∫£n ph·∫©m chuy√™n d·ª•ng
   - N·∫∂NG: V·∫•n ƒë·ªÅ lan r·ªông, vi√™m nhi·ªÅu, c·∫ßn ƒëi·ªÅu tr·ªã t√≠ch c·ª±c
   - R·∫§T N·∫∂NG: Vi√™m tr·∫ßm tr·ªçng, s·∫πo nhi·ªÅu, c·∫ßn g·∫∑p b√°c sƒ© da li·ªÖu

4. G·ª¢I √ù: (1 c√¢u ng·∫Øn)

QUAN TR·ªåNG: Ph·∫£i ghi r√µ M·ª®C ƒê·ªò (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG).

Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, b·∫±ng ti·∫øng Vi·ªát."""

        if note:
             vision_prompt += f"\n\nGhi ch√∫ th√™m t·ª´ ng∆∞·ªùi d√πng: {note}"
        
        # G·ªçi vision model
        response = vision_model.generate_content([vision_prompt, img])
        analysis = response.text
        
        print("‚úÖ ƒê√£ ph√¢n t√≠ch xong!")
        
        return analysis
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ph√¢n t√≠ch ·∫£nh: {str(e)}")
        return None

# =============================================================================
# INTERACTIVE CHAT (CLI)
# =============================================================================
def chat(rag_chain):
    """Interactive chat trong terminal v·ªõi h·ªó tr·ª£ ph√¢n t√≠ch ·∫£nh da v√† l∆∞u l·ªãch s·ª≠"""
    print("\n" + "=" * 80)
    print("üí¨ COSMETIC CONSULTANT CHATBOT (‚ö° RAG + üì∏ VLM + üíæ HISTORY)")
    print("=" * 80)
    
    # Load l·ªãch s·ª≠ chat tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥)
    previous_history = load_latest_chat_history()
    if previous_history:
        history, history_file = previous_history
        print(f"\nüìñ T√¨m th·∫•y l·ªãch s·ª≠ chat tr∆∞·ªõc: {history_file.name}")
        print(f"    S·ªë l∆∞·ª£ng: {len(history)} tin nh·∫Øn")
        view = input("    Xem l·ªãch s·ª≠? (y/n): ").strip().lower()
        if view == 'y':
            print("\n" + "=" * 80)
            print("üìú L·ªäCH S·ª¨ CHAT TR∆Ø·ªöC:")
            print("=" * 80)
            for msg in history[-10:]:  # Hi·ªÉn th·ªã 10 tin nh·∫Øn cu·ªëi
                role = "üßë B·∫°n" if msg['role'] == 'user' else "ü§ñ Bot"
                content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
                print(f"{role}: {content}")
                print("-" * 40)
            print("=" * 80)
    
    print("\nüìù G√µ c√¢u h·ªèi c·ªßa b·∫°n v√† nh·∫•n Enter")
    print("üí° V√≠ d·ª• text: 'T√¥i c·∫ßn kem d∆∞·ª°ng cho da kh√¥ nh·∫°y c·∫£m'")
    print("üì∏ Ph√¢n t√≠ch ·∫£nh DA: G·ª≠i ƒë∆∞·ªùng d·∫´n ·∫£nh da c·ªßa b·∫°n (t·ª± ƒë·ªông nh·∫≠n di·ªán)")
    print("    ‚Üí VLM ph√¢n t√≠ch chi ti·∫øt t√¨nh tr·∫°ng da")
    print("    ‚Üí RAG t∆∞ v·∫•n s·∫£n ph·∫©m ph√π h·ª£p d·ª±a tr√™n ph√¢n t√≠ch")
    print("    V√≠ d·ª•: C:\\Users\\Photos\\my_skin.jpg")
    print("üö™ G√µ 'exit', 'quit' ho·∫∑c 'tho√°t' ƒë·ªÉ k·∫øt th√∫c v√† L∆ØU L·ªäCH S·ª¨")
    print("‚ö° C√¥ng ngh·ªá: VLM (Gemini 2.5 Flash) + RAG (ChromaDB)\n")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o l·ªãch s·ª≠ chat m·ªõi v√† conversation memory
    chat_history = {
        'session_start': datetime.now().isoformat(),
        'messages': []
    }
    
    # Conversation memory - l∆∞u context trong phi√™n (bot s·∫Ω nh·ªõ!)
    conversation_context = []  # L∆∞u t·∫•t c·∫£ trao ƒë·ªïi: [(user_msg, bot_response), ...]
    
    # C√°c ƒëu√¥i file ·∫£nh ƒë∆∞·ª£c h·ªó tr·ª£
    IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.gif', '.tiff')
    
    while True:
        print()
        try:
            # Nh·∫≠n input t·ª´ user
            question = input("üßë B·∫°n: ").strip()
            
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán tho√°t
            if not question:
                print("‚ö†Ô∏è  Vui l√≤ng nh·∫≠p c√¢u h·ªèi!")
                continue
                
            if question.lower() in ['exit', 'quit', 'tho√°t', 'bye', 'goodbye']:
                print("\nüëã C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª•!")
                # L∆∞u l·ªãch s·ª≠ tr∆∞·ªõc khi tho√°t
                if chat_history['messages']:
                    chat_history['session_end'] = datetime.now().isoformat()
                    save_chat_history(chat_history)
                print("=" * 80)
                break
            
            # T·ª± ƒë·ªông nh·∫≠n di·ªán ƒë∆∞·ªùng d·∫´n ·∫£nh
            question_clean = question.strip('"').strip("'")
            image_path_candidate = None
            text_question = None
            
            # Logic t√¨m ƒë∆∞·ªùng d·∫´n ·∫£nh (gi·ªØ nguy√™n t·ª´ file m·ªõi)
            if '"' in question:
                matches = re.findall(r'"([^"]+)"', question)
                for match in matches:
                    if any(match.lower().endswith(ext) for ext in IMAGE_EXTENSIONS):
                        image_path_candidate = match
                        text_question = question.replace(f'"{match}"', '').strip()
                        break
            
            if not image_path_candidate:
                words = question_clean.split()
                for word in words:
                    if any(word.lower().endswith(ext) for ext in IMAGE_EXTENSIONS):
                        if '\\' in word or '/' in word or ':' in word:
                            image_path_candidate = word
                            text_question = question_clean.replace(word, '').strip()
                            break
            
            if not image_path_candidate and any(question_clean.lower().endswith(ext) for ext in IMAGE_EXTENSIONS):
                image_path_candidate = question_clean
            
            if not image_path_candidate and question.lower().startswith(('image:', '·∫£nh:', 'anh:')):
                parts = question.split(':', 1)
                if len(parts) > 1:
                    image_path_candidate = parts[1].strip().strip('"').strip("'")
            
            # X·ª≠ l√Ω n·∫øu t√¨m th·∫•y ·∫£nh
            if image_path_candidate:
                image_path = image_path_candidate
                if not os.path.isabs(image_path):
                    image_path = os.path.join(os.getcwd(), image_path)
                
                if not os.path.exists(image_path):
                    print(f"‚ùå Kh√¥ng t√¨m th·∫•y file ·∫£nh: {image_path}")
                    print("üí° Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n!")
                    print("-" * 80)
                    continue
                
                # VLM ph√¢n t√≠ch ·∫£nh da
                skin_analysis = analyze_skin_image(image_path)
                
                chat_history['messages'].append({
                    'timestamp': datetime.now().isoformat(),
                    'role': 'user',
                    'type': 'image',
                    'content': f"[G·ª≠i ·∫£nh: {os.path.basename(image_path)}]",
                    'image_path': image_path,
                    'additional_text': text_question if text_question else None
                })
                
                if skin_analysis:
                    analysis_upper = skin_analysis.upper()
                    is_very_severe = 'R·∫§T N·∫∂NG' in analysis_upper or 'R·∫§T NGHI√äM TR·ªåNG' in analysis_upper
                    
                    if is_very_severe:
                        print("\n" + "‚ö†Ô∏è " * 20)
                        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: T√åNH TR·∫†NG DA R·∫§T NGHI√äM TR·ªåNG!")
                        print("‚ö†Ô∏è " * 20)
                        print("\nüè• KHUY·∫æN C√ÅO:")
                        print("    ‚Ä¢ T√¨nh tr·∫°ng da c·ªßa b·∫°n C·∫¶N ƒë∆∞·ª£c b√°c sƒ© da li·ªÖu thƒÉm kh√°m")
                        print("    ‚Ä¢ Kh√¥ng n√™n t·ª± ƒëi·ªÅu tr·ªã ho·∫∑c ch·ªâ d√πng m·ªπ ph·∫©m")
                        print("    ‚Ä¢ Vui l√≤ng ƒë·∫∑t l·ªãch g·∫∑p b√°c sƒ© da li·ªÖu NGAY")
                        print("\n" + "=" * 80)
                        print("\nüí° Tuy nhi√™n, d∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë s·∫£n ph·∫©m H·ªñ TR·ª¢ (KH√îNG THAY TH·∫æ ƒëi·ªÅu tr·ªã y khoa):\n")
                    
                    # T·∫°o query RAG d·ª±a tr√™n ph√¢n t√≠ch
                    if text_question:
                        if is_very_severe:
                            rag_query = f"""T√¨nh tr·∫°ng da (R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®):
{skin_analysis}

Y√™u c·∫ßu: {text_question}

G·ª£i √Ω 1-2 s·∫£n ph·∫©m H·ªñ TR·ª¢ NH·∫∏ NH√ÄNG (kh√¥ng thay th·∫ø ƒëi·ªÅu tr·ªã y khoa). 
NH·∫§N M·∫†NH: C·∫ßn g·∫∑p b√°c sƒ© da li·ªÖu."""
                        else:
                            rag_query = f"""T√¨nh tr·∫°ng da (t·ª´ ph√¢n t√≠ch ·∫£nh):
{skin_analysis}

Y√™u c·∫ßu: {text_question}

T∆∞ v·∫•n 2-3 s·∫£n ph·∫©m C·ª§ TH·ªÇ ph√π h·ª£p v·ªõi M·ª®C ƒê·ªò."""
                    else:
                         if is_very_severe:
                            rag_query = f"""T√¨nh tr·∫°ng da (R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®):
{skin_analysis}

G·ª£i √Ω 1-2 s·∫£n ph·∫©m H·ªñ TR·ª¢ NH·∫∏ NH√ÄNG (kh√¥ng thay th·∫ø ƒëi·ªÅu tr·ªã y khoa).
NH·∫§N M·∫†NH: C·∫ßn g·∫∑p b√°c sƒ© da li·ªÖu."""
                         else:
                            rag_query = f"""T√¨nh tr·∫°ng da (t·ª´ ph√¢n t√≠ch ·∫£nh):
{skin_analysis}

T∆∞ v·∫•n 2-3 s·∫£n ph·∫©m C·ª§ TH·ªÇ ph√π h·ª£p v·ªõi M·ª®C ƒê·ªò."""
                    
                    print("\nüîé T√¨m s·∫£n ph·∫©m d·ª±a tr√™n m·ª©c ƒë·ªô nghi√™m tr·ªçng...")
                    time.sleep(1)
                    
                    product_recommendation = rag_chain.invoke(rag_query)
                    
                    user_input_desc = f"[G·ª≠i ·∫£nh da] {text_question if text_question else 'Ph√¢n t√≠ch v√† t∆∞ v·∫•n'}"
                    conversation_context.append((user_input_desc, product_recommendation))
                    
                    print("\nüíÑ T∆Ø V·∫§N S·∫¢N PH·∫®M:")
                    print("=" * 80)
                    print(product_recommendation)
                    print("=" * 80)
                    
                    bot_response = product_recommendation
                    if is_very_severe:
                        bot_response = f"‚ö†Ô∏è C·∫¢NH B√ÅO: R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®!\n\n{product_recommendation}"
                    
                    chat_history['messages'].append({
                        'timestamp': datetime.now().isoformat(),
                        'role': 'assistant',
                        'type': 'product_recommendation',
                        'content': bot_response,
                        'skin_analysis': skin_analysis,
                        'severity': 'VERY_SEVERE' if is_very_severe else 'NORMAL'
                    })
                    
                    if is_very_severe:
                        print("\n" + "‚ö†Ô∏è " * 20)
                        print("‚ö†Ô∏è  L∆ØU √ù: C√°c s·∫£n ph·∫©m tr√™n CH·ªà H·ªñ TR·ª¢, KH√îNG THAY TH·∫æ ƒëi·ªÅu tr·ªã y khoa!")
                        print("‚ö†Ô∏è  VUI L√íNG ƒê·∫∂T L·ªäCH G·∫∂P B√ÅC Sƒ® DA LI·ªÑU NGAY! üè•")
                        print("‚ö†Ô∏è " * 20)
                
                print("-" * 80)
                continue
            
            # X·ª≠ l√Ω c√¢u h·ªèi text th√¥ng th∆∞·ªùng
            print("\n‚è≥ ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi...")
            start_time = time.time()
            
            chat_history['messages'].append({
                'timestamp': datetime.now().isoformat(),
                'role': 'user',
                'type': 'text',
                'content': question
            })
            
            # PH√ÅT HI·ªÜN B·ªÜNH DA v√† √°nh x·∫° sang lo·∫°i da ph√π h·ª£p
            detected_condition, suitable_skin_types = detect_skin_condition_and_types(question)
            
            if detected_condition:
                print(f"\nü©∫ Ph√°t hi·ªán b·ªánh da: {detected_condition.upper()}")
                print(f"üìã Lo·∫°i da ph√π h·ª£p: {', '.join(suitable_skin_types)}")
                
                skin_types_mapping = {
                    "Kh√¥": "Dry", "Th∆∞·ªùng": "Normal", "D·∫ßu": "Oily",
                    "Nh·∫°y c·∫£m": "Sensitive", "H·ªón h·ª£p": "Combination"
                }
                english_skin_types = [skin_types_mapping.get(st, st) for st in suitable_skin_types]
                skin_query = " ".join(english_skin_types)
                enhanced_query = f"{detected_condition} {skin_query} skin treatment moisturizer serum toner cream"
                
                query_to_use = enhanced_query
                print(f"üîç Query t√¨m ki·∫øm: {query_to_use}")
            else:
                query_to_use = question
            
            time.sleep(1)
            
            # Logic x·ª≠ l√Ω Context
            if detected_condition:
                # C√≥ b·ªánh da ‚Üí Query tr·ª±c ti·∫øp, KH√îNG d√πng context c≈©
                response = rag_chain.invoke(query_to_use)
            elif conversation_context:
                # Kh√¥ng c√≥ b·ªánh da + c√≥ context
                recent_context = conversation_context[-3:]
                context_str = "\n".join([
                    f"User ƒë√£ h·ªèi: {ctx[0]}\nBot ƒë√£ tr·∫£ l·ªùi: {ctx[1][:200]}..." 
                    for ctx in recent_context
                ])
                
                query_with_context = f"""L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{context_str}

C√ÇU H·ªéI HI·ªÜN T·∫†I: {query_to_use}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n c√¢u h·ªèi hi·ªán t·∫°i. Ch·ªâ tham kh·∫£o l·ªãch s·ª≠ n·∫øu user ƒëang h·ªèi ti·∫øp v·ªÅ c√πng topic."""
                response = rag_chain.invoke(query_with_context)
            else:
                response = rag_chain.invoke(query_to_use)
            
            elapsed_time = time.time() - start_time
            conversation_context.append((question, response))
            
            print(f"\nü§ñ Bot: {response}")
            print(f"\n‚ö° Th·ªùi gian ph·∫£n h·ªìi: {elapsed_time:.2f}s")
            print("-" * 80)
            
            chat_history['messages'].append({
                'timestamp': datetime.now().isoformat(),
                'role': 'assistant',
                'type': 'text',
                'content': response,
                'response_time': elapsed_time,
                'detected_condition': detected_condition if detected_condition else None,
                'suitable_skin_types': suitable_skin_types if suitable_skin_types else None
            })
            
        except KeyboardInterrupt:
            print("\n\nüëã ƒê√£ nh·∫≠n t√≠n hi·ªáu tho√°t. C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng!")
            print("=" * 80)
            break
            
        except Exception as e:
            print(f"\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {str(e)}")
            print("üí° Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c!")
            print("-" * 80)

# =============================================================================
# HELPER FUNCTIONS CHO BACKEND (GI·ªÆ L·∫†I T·ª™ FILE C≈®)
# =============================================================================

def is_supported_condition(condition):
    """Ki·ªÉm tra b·ªánh da c√≥ trong danh s√°ch h·ªó tr·ª£ kh√¥ng (T·ª´ file c≈©)"""
    if not condition:
        return False
    condition_lower = condition.lower()
    return any(supported in condition_lower or condition_lower in supported 
               for supported in SUPPORTED_SKIN_CONDITIONS)

def check_severity(analysis: str) -> bool:
    """Check if skin condition is severe (T·ª´ file c≈©)"""
    if not analysis:
        return False
    return any(keyword in analysis.upper() for keyword in ['R·∫§T N·∫∂NG', 'R·∫§T NGHI√äM TR·ªåNG'])

def analyze_with_context(question: str, conversation_history: list = None) -> str:
    """
    Analyze question with conversation context + Skin Condition Logic (T·ª´ file c≈©)
    ‚ö†Ô∏è N·∫æU PH√ÅT HI·ªÜN B·ªÜNH DA ‚Üí B·ªé QUA CONTEXT ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    """
    # Detect skin condition
    detected_condition, suitable_skin_types = detect_skin_condition_and_types(question)
    
    if detected_condition:
        # C√ì B·ªÜNH DA ‚Üí Kh√¥ng d√πng context, mapping sang ti·∫øng Anh
        skin_types_mapping = {
            "Kh√¥": "Dry",
            "Th∆∞·ªùng": "Normal",
            "D·∫ßu": "Oily",
            "Nh·∫°y c·∫£m": "Sensitive",
            "H·ªón h·ª£p": "Combination"
        }
        
        english_skin_types = [skin_types_mapping.get(st, st) for st in suitable_skin_types]
        skin_query = " ".join(english_skin_types)
        enhanced_query = f"{detected_condition} {skin_query} skin treatment moisturizer serum toner cream"
        
        return enhanced_query
    
    # KH√îNG C√ì B·ªÜNH DA ‚Üí D√πng context b√¨nh th∆∞·ªùng
    context_str = ""
    if conversation_history:
        recent_context = conversation_history[-3:]
        context_str = "L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:\n" + "\n".join([
            f"User: {ctx[0]}\nBot: {ctx[1][:200]}..."
            for ctx in recent_context
        ])

    return f"""{context_str}
C√ÇU H·ªéI HI·ªÜN T·∫†I: {question}
H√£y tr·∫£ l·ªùi d·ª±a tr√™n c√¢u h·ªèi hi·ªán t·∫°i. Ch·ªâ tham kh·∫£o l·ªãch s·ª≠ n·∫øu user ƒëang h·ªèi ti·∫øp v·ªÅ c√πng topic."""

def build_image_analysis_query(skin_analysis: str, additional_text: str = None) -> str:
    """Build RAG query based on Image Analysis Result with severity awareness (T·ª´ file c≈©)"""
    is_severe = any(keyword in skin_analysis.upper() for keyword in ['R·∫§T N·∫∂NG', 'R·∫§T NGHI√äM TR·ªåNG', 'C·∫¶N G·∫∂P B√ÅC Sƒ®'])
    
    warning = "(R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®)" if is_severe else "(t·ª´ ph√¢n t√≠ch ·∫£nh)"
    advice_req = "G·ª£i √Ω 1-2 s·∫£n ph·∫©m H·ªñ TR·ª¢ NH·∫∏ NH√ÄNG. NH·∫§N M·∫†NH: C·∫ßn g·∫∑p b√°c sƒ©." if is_severe else "T∆∞ v·∫•n 2-3 s·∫£n ph·∫©m C·ª§ TH·ªÇ ph√π h·ª£p v·ªõi M·ª®C ƒê·ªò."
    
    user_req = f"\nY√™u c·∫ßu th√™m c·ªßa user: {additional_text}" if additional_text else ""
    
    return f"""T√¨nh tr·∫°ng da {warning}:
{skin_analysis}
{user_req}
{advice_req}"""

def get_product_suggestions_by_skin_types(db, skin_types: list, num_products: int = 5) -> list:
    """
    Truy v·∫•n s·∫£n ph·∫©m ph√π h·ª£p v·ªõi lo·∫°i da (bilingual search) (T·ª´ file c≈©)
    Returns: list of product names
    """
    if not db or not skin_types:
        print("‚ö†Ô∏è No database or skin types provided")
        return []
    
    try:
        print(f"üîç Searching products for skin types: {skin_types}")
        
        # Map ti·∫øng Vi·ªát sang ti·∫øng Anh
        vietnamese_to_english = {
            "Kh√¥": "Dry",
            "Th∆∞·ªùng": "Normal",
            "D·∫ßu": "Oily",
            "H·ªón h·ª£p": "Combination",
            "Nh·∫°y c·∫£m": "Sensitive"
        }
        
        # T·∫°o search terms (c·∫£ VN v√† EN)
        search_terms = []
        for skin_type in skin_types:
            search_terms.append(skin_type)
            if skin_type in vietnamese_to_english:
                search_terms.append(vietnamese_to_english[skin_type])
        
        print(f"üîç Search terms (VN + EN): {search_terms}")
        
        query = f"s·∫£n ph·∫©m chƒÉm s√≥c da {' '.join(search_terms)}"
        
        retriever = db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": num_products * 5,
                "fetch_k": num_products * 10,
                "lambda_mult": 0.5
            }
        )
        
        docs = retriever.invoke(query)
        print(f"üìö Retrieved {len(docs)} documents from vector store")
        
        product_names = []
        seen_products = set()
        
        for doc in docs:
            product_name = doc.metadata.get('product_name')
            
            if not product_name:
                content_lines = doc.page_content.split('\n')
                for line in content_lines:
                    if 'Product Name:' in line:
                        product_name = line.split(':', 1)[1].strip()
                        break
            
            if product_name and product_name not in seen_products:
                content_lower = doc.page_content.lower()
                metadata_str = str(doc.metadata).lower()
                
                match = any(
                    term.lower() in content_lower or
                    term.lower() in metadata_str
                    for term in search_terms
                )
                
                if match:
                    product_names.append(product_name)
                    seen_products.add(product_name)
                    print(f"‚úì Found: {product_name}")
                    
                    if len(product_names) >= num_products:
                        break
        
        # Fallback: add general products if not enough
        if len(product_names) < num_products:
            print(f"‚ö†Ô∏è Only found {len(product_names)} matching products, adding general...")
            for doc in docs:
                product_name = doc.metadata.get('product_name')
                if not product_name:
                    content_lines = doc.page_content.split('\n')
                    for line in content_lines:
                        if 'Product Name:' in line:
                            product_name = line.split(':', 1)[1].strip()
                            break
                
                if product_name and product_name not in seen_products:
                    product_names.append(product_name)
                    seen_products.add(product_name)
                    print(f"‚úì Added general: {product_name}")
                    if len(product_names) >= num_products:
                        break
        
        print(f"‚úÖ Returning {len(product_names)} product suggestions")
        return product_names[:num_products]
        
    except Exception as e:
        print(f"‚ùå Error getting product suggestions: {e}")
        return []

def map_disease_to_skin_types(disease_class: str) -> list:
    """Map disease class sang skin types ph√π h·ª£p (T·ª´ file c≈©)"""
    print(f"üîç Mapping disease: {disease_class}")
    
    disease_lower = disease_class.lower().replace('_', ' ')
    
    disease_mapping = {
        'acne': ['H·ªón h·ª£p', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'actinic keratosis': ['Kh√¥', 'Th∆∞·ªùng'],
        'drug eruption': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'eczema': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'psoriasis': ['Kh√¥'],
        'rosacea': ['H·ªón h·ª£p', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'seborrh keratoses': ['Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'sun sunlight damage': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'Nh·∫°y c·∫£m'],
        'tinea': ['H·ªón h·ª£p', 'D·∫ßu'],
        'warts': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'normal': ['Th∆∞·ªùng']
    }
    
    for key, skin_types in disease_mapping.items():
        if key in disease_lower or disease_lower in key:
            print(f"‚úì Mapped to skin types: {skin_types}")
            return skin_types
    
    # Fallback to SKIN_CONDITION_TO_SKIN_TYPE
    for condition_key, skin_types in SKIN_CONDITION_TO_SKIN_TYPE.items():
        if condition_key in disease_lower or disease_lower in condition_key:
            print(f"‚úì Mapped via SKIN_CONDITION_TO_SKIN_TYPE: {skin_types}")
            return skin_types
    
    print(f"‚ö†Ô∏è No specific mapping found, using default")
    return ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"]

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    """Main function - ƒëi·ªÉm kh·ªüi ƒë·∫ßu ch∆∞∆°ng tr√¨nh"""
    try:
        print("\nüéØ COSMETIC RAG CHATBOT - INTERACTIVE MODE")
        print("=" * 80)
        
        # 1. Setup API Key
        setup_api_key()
        
        # 2. Load/Create Vector Store
        db, embeddings = load_or_create_vectorstore()
        
        if db is None:
            print("\n‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o vector store. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh!")
            return 1
        
        # 3. Setup RAG Chain
        rag_chain = setup_rag_chain(db)
        
        if rag_chain is None:
            print("\n‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o RAG chain. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh!")
            return 1
        
        # 4. Start Chat
        chat(rag_chain)
        
    except Exception as e:
        print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG: {str(e)}")
        print("üí° Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† th·ª≠ l·∫°i!")
        return 1
    
    return 0

# =============================================================================
# ENTRY POINT
# =============================================================================
if __name__ == "__main__":
    exit(main())